# YAML configuration 
MODEL:
  name: "distilbert"  
  suffix: ""
  num_pre_output: 768 # Number of pre last layer output for classification 
  model_id: "distilbert-base-uncased"
  train_size: .8
  max_len: 128
  train_batch_size: 4
  valid_batch_size: 4
  learning_rate: 1.0e-05
  epochs: 10

train_params:
  batch_size: 4
  shuffle: True
  num_workers: 0
                    

test_params:
  batch_size: 4
  shuffle: True
  num_workers: 0
                    

pre_processing:
  synonym_fold: 1
  deletion_fold: 1